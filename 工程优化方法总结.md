# 工程优化方法总结



## 第二章 基础知识



### 多元函数的梯度



以f的n个偏导数为分量的向量称为f在x处的梯度，记为

$$
\nabla f(x)=(\frac{\partial f(x)}{\partial x_1},\frac{\partial f(x)}{\partial x_1},...\frac{\partial f(x)}{\partial x_1})^T
$$
也称作f关于x的一阶导数。



梯度方向是函数具有最大变化率的方向，**即：沿地图方向的方向导数最大。**



### 方向导数


$$
\frac{\partial f(x_0)}{\partial p}=\lim_{t \to 0^+}\frac{f(x_0+p)-f(x_0)}{t}=\lim_{t \to 0^+}\frac{f(x_0+te)-f(x_0)}{t}
$$



当方向导数大于0时，表示$f(x)$在$x_0$处沿$p$方向上升；反之，当方向导数小于0时，表示$f(x)$在$x_0$处沿$p$方向下降。



函数升降速度的快慢有方向导数的绝对值大小来决定，绝对值越大生姜速度越大，也称为$f(x)$在$x_0$处沿$p$方向的变化率。



**推论：**若$\nabla f(x_0)^T p<0$，则$p$是函数$f(x)$在$x_0$处的下降方向。

​			若$\nabla f(x_0)^T p>0$，则$p$是函数$f(x)$在$x_0$处的上升方向。

通过这个推论，我们可以判断在某一点处沿某一方向是上升方向还是下降方向。



### 多元函数的Hessian矩阵



多元函数的二阶导数即为Hessian矩阵$\nabla^2f(x)$
$$
\nabla^2f(x)=\nabla(\nabla f(x))= \left[
\begin{matrix}
\frac{\partial^2f(x)}{\partial x_1^2} & \frac{\partial^2f(x)}{\partial x_1 \partial x_2} & ...
\frac{\partial^2f(x)}{\partial x_1 \partial x_n} \\
\frac{\partial^2f(x)}{\partial x_2 \partial x_1} & \frac{\partial^2f(x)}{\partial x_2^2} & ...
\frac{\partial^2f(x)}{\partial x_2 \partial x_n} \\
\frac{\partial^2f(x)}{\partial x_n \partial x_1} &
\frac{\partial^2f(x)}{\partial x_n \partial x_2} & ...
\frac{\partial^2f(x)}{\partial x_n^2}

\end{matrix}
\right]
$$


使用顺序主子式来判断Hessian矩阵是否正定。



### 泰勒展开



以二元函数在某一点处的二阶泰勒展开为例
$$
f(x_1+\delta_1,x_2+\delta_2)=f(x_1,x_2)+\frac{\partial f}{\partial x_1}\delta_1+\frac{\partial f}{\partial x_2}\delta_2 + \frac{1}{2}(\frac{\partial^2 f}{\partial x_1^2}\delta_1^2 + \frac{2\partial^2 f}{\partial x_1\partial x_2}\delta_1 \delta_2+\frac{\partial^2 f}{\partial x_2^2}\delta_2^2) + o(\Vert \delta \Vert^2)
$$



### 可行方向



给定可行域$\Omega$内的一点$x$，若存在一个常量$\alpha>0$，对于任意一个方向向量，有$x+\alpha d\in\Omega$，对所有的$0\leq \alpha\leq\hat{\alpha}$都成立，那么我们成$d$为点$x$处的一个可行方向。



### 二元函数的极值判别条件



#### 一阶必要条件



1、若目标函数连续可导，点$x^*$式一个局部极小值点，那么有：
$$
g(x^*)\cdot d \geq0
$$
其中，d是点$x^*$处的任意一个可行方向。

2、若$x^*$位于可行域$\Omega$内部（意味着任意方向均为可行方向）,那么有
$$
g(x^*)=0
$$
即，若极小值点位于边界，那么其任意方向均为上升方向。而若极小值点位于可行域内部，则其一阶导数为0。



#### 二阶必要条件



1、若$f(x)$二阶连续可到，那么当$x^*$是一个局部极小值点时，对于$x^*$处的任一可行方向：

1）$g(x^*)^T\cdot d\ge0$

2)若$g(x^*)^T\cdot d=0$，那么有$d^TH(x^*)d\ge0$。

2、若$x^*$是可行域上的内点，那么，$x^*$是一个极小值点的必要条件是：

1)$g(x^*)=0$

2)$d^TH(x^*)d\ge 0$对于所有的向量$d$。

即所有的二阶导与方向的乘积均要大于等于0。



#### 二阶充分条件



若$f(x)$二阶连续可导，点$x^*$位于优化问题可行域的内部，则若下列两个条件成立，$x^*$是一个强局部最小点。

1、$g(x^*)=0$

2、$H(x^*)$正定



### 凸集



对于集合$R_c$中的任意两点$x_1,x_2$以及任意$\alpha$满足$0<\alpha<1$，若点$x=\alpha\cdot x_1+(1-\alpha)\cdot x_2$依然在$R_c$中，那么我们称$R_c$为凸集。

设A,B是凸集，则$A\cap B、A+B、A-B$也是凸集，**而$A\cup B$不一定是凸集。**



### 凸函数



设集合D为凸集，函数$f:D\rightarrow R$对于任意两点$x_1,x_2$和$0<\alpha<1$，有
$$
f(\alpha\cdot x_1+(1-\alpha)\cdot x_2)\le\alpha\cdot f(x_1)+(1-\alpha)\cdot f(x_2)
$$
成立，则成$f(x)$为凸集D上的凸函数。



### 凸函数的性质



#### 凸函数与梯度的关系



设$f(x)$在凸集$R_c$上一阶连续可到，那么$f(x)$是定义在凸集$R_C$上的凸函数当且仅当：
$$
f(x_1)\ge f(x) + g(x)^T(x_1-x)
$$
对所有的$x,x_1\in R_c$都成立，这里$g(x)=\nabla f(x)$



#### 凸函数与Hessian矩阵的关系



设$f(x)$在凸集$R_c$上二阶连续可导，$f(x)$是凸函数当且仅当对于的Hessian矩阵式半正定的。



### 凸规划问题的性质



设$f(x)$式定义在凸集$R_c$上的凸函数，那么：

$f(x)$的局部极小值点即为全局极小值点。



## 第三章 优化算法的一般特性



### 下降函数



假设一系列点序列是由某一算法A产生的，我们想找到一个连续的实值函数$D(x)$，使$D(x_{k+1})\le D(x_k)$，称$D(x)$为下降函数。（自己的理解）



定义：

设$S\subset X$包含问题的解，假定算法A是定义在空间X上的算法，即$x_{k+1}=A(x_k)$，我们成一个连续的实值函数$D(x)$为下降函数若该函数满足一下条件

1、若$x_k\notin S$，对于$x_{x+1}\in A(x_k)$，有$D(x_{k+!})<D(x_k)$；

2、若$x_k\in S$，对于$x_{k+1}\in A(x_k)$，有$D(x_{k+1})\le D(x_k)$。



注意：要区分算法A和下降函数D，算法A是产生一系列点序列的函数，而下降函数是在该序列上满足下降条件的函数。



### 算法收敛性

由算法A产生的点序列随着迭代次数的无线增加，点序列趋近于一个点。类似于极限的概念。（自己的理解）



定义：

设A为空间X上的一个算法，那么从初始点$x_0$出发，可由$x_{k+1}\in A(x_k)$得到一个无限长序列$\{x_k\}_{k=0}^\infty$。假如对应的优化问题的解集S和下降函数$D(x)$存在，使得以下条件成立，则$\{x_k\}_{k=0}^\infty$极限收敛到解集中一点。

1、所有$x_k$都包含在X的一个闭的、有界子集内；

2、$D(x_k)$为下降函数；

3、算法A在$x\in X$和$x\notin S$内是封闭的。



注意：验证算法收敛性注意三点，点集封闭，算法封闭，存在下降函数。



### 收敛阶次

收敛阶次定义在优化算法产生的无限序列$\{x_k\}_{k=0}^\infty$上。针对该无限序列，收敛阶次的定义满足
$$
0\le\beta<\infty 其中\beta =\lim_{k\rightarrow\infty}\frac{\vert x_{k+1}-\hat{x}\vert}{\vert x_k-\hat{x}\vert^p}
$$


的最大非负整数p，同时称$\beta$为收敛率。

注意：$\hat{x}$为序列收敛的点，一般都能一眼看出来。



## 第四章 一维函数优化



### 二分法



从搜索区间$[a,b]$的中点$x_c$出发，找$x_c-\varepsilon、x_c+\varepsilon$，其中$\varepsilon>0$，通过比较$f(x_c-\varepsilon)、f(x_c+\varepsilon)$的函数值，来缩小区间，不断重复，直到区间长度满足搜索精度要求。



#### 二分法算法步骤



已知：单峰函数$f(x)$，区间$[x_L,x_U]$包含函数的极值点。

1、计算极值点所在区间中点，记为$x_c$

2、计算函数在$f(x_c-\varepsilon)$和$f(x_c+\varepsilon)$处的函数值，其中$\varepsilon>0$

3、由$f(x_c-\varepsilon)$和$f(x_c+\varepsilon)$的大小关系，缩小极值点所在的区间

4、重复上述步骤，直到区间长度满足精度要求

注意：这里$\varepsilon$很小



每一次迭代都使得区间的长度减半，这样经过k次迭代，区间长度降为：
$$
I_k=(\frac{1}{2})^k\cdot I_0
$$


#### 三种停止条件



1、极值点的精度：$\vert x-x^*\vert\le\zeta$；其中$x^*$为准确的最小值点，$\zeta$为设定的误差容限。（该条件需要预先知道最小值点，一般用$\vert x_{k+1}-x_k\vert\le\zeta$）

2、极值的精度：$\vert f(x_{k+1})-f(x_k)\vert\le\varepsilon$（理想：$\vert f(x_{k+1})-f(x^*)\vert\le\varepsilon$）

3、迭代次数：即当迭代次数满足一定数目时，迭代停止



### 斐波那契法



第一次在区间$[a,b]$内找两个点$x_1、x_2$，使得$x_2-a=b-x_1$。通过比较$f(x_1)、f(x_2)$的函数值来缩小区间。假设我们将$[a,x_1]$部分舍弃，现在有$x_1、x_2、b$三个点，令$a=x_1、x_1=x_2、b=b$，下次迭代只需找一个新点$x_2$，使得新的四个点满足$x_2-a=b-x_1$即可。

二分法每次迭代都需要找区间内两点，计算两点的函数值。斐波那契法只在第一次迭代时找区间内两点，余下迭代时只需新计算一个点的函数值。



#### 斐波那契法算法步骤



假设第k次迭代，区间为$[x_{Lk},x_{Uk}]$，长度为$I_k$。

1、在区间$[x_{Lk},x_{Uk}]$内取两点$x_{ak},x_{bk}$，使得区间$[x_{Lk},x_{bk}]$和$[x_{ak},x_{Uk}]$长度相等，即：$I_{k+1}^L=I_{k+1}^R$

2、$x_{ak}$或$x_{bk}$刚好为下一次迭代区间中的一个点

3、上述迭代过程满足：$I_k=I_{k+1}+I_{k+2}$，即迭代的区间长度应满足斐波那契数列。



#### 斐波那契法的特点



1、迭代开始前，就要根据区间长度和误差容限确定斐波那契数列的长度，确定斐波那契数列长度的依据：
$$
I_n=\frac{I_1}{F_n}
$$
2、如果优化问题的停止条件是类似$\vert f(x_{k+1})-f(x_k)\vert\le\varepsilon$，那么斐波那契法不太好处理。

3、斐波那契法收敛到相同精度的极值点，需要计算函数值的点数比二分法少，但是迭代的次数多。



### 黄金分割法



第一次在搜索区间$[a,b]$内找两个点$x_1=b-0.618(a-b)、x_2=a+0.618(a-b)$，分别计算$f(x_1)、f(x_2)$的函数值，通过比较函数值来缩小区间，并使上次迭代时的两个点中的一点成为下次迭代时的一个点。（思想跟斐波那契法差不多，区别在于找点的方法）



#### 黄金分隔法算法步骤



1、初始化：函数$f(x)$，包含极值点的区间$[x_{L，1}，x_{U,1}]$，误差限$\varepsilon$。

2、计算：$I_1=x_{U,1}-x_{L,1}，I_2=I_1/K，x_{a,1}=x_{U,1}-I_2，x_{b,1}=x_{L,1}-I_2$

3、计算：$I_{k+2}=I_{k+1}/K$，根据$f(x_{L,k})，f(x_{a,k})，f(x_b,k)，f(x_{U,k})$的值更新$x_{L,k+1}，x_{a,k+1}，x_{b,k+1}，x_{U,k+1}$。

4、若$I_k<\varepsilon$，结束迭代过程，计算：
$$
x^*=
\begin{cases}
\frac{1}{2}(x_{a,k+1}+x_{U,k+1})(f_{a,k+1>f_{b,k+1}})\\
\frac{1}{2}(x_{a,k+1}+x_{b,k+1})(f_{a,k+1=f_{b,k+1}})\\
\frac{1}{2}(x_{L,k+1}+x_{b,k+1})(f_{a,k+1<f_{b,k+1}})\\
\end{cases}
$$
(其实只要满足了误差容限，区间内任意一点都能够代表极小值点)

5、设置$k=k+1$，转至第三步。



### 二次内插方法



通过搜索区间内选三个点，来拟合二次曲线，通过二次曲线的极值来进行迭代。



#### 二次内插法基本思想



已知$x_1，x_2，x_3$以及相应的函数值，可以用二次函数近似目标曲线，通过解二次函数极小值近似原函数极小值。



#### 二次内插法步骤

1、输入$x_1，x_3$以及误差容限$\varepsilon$；设$\bar{x_0}=10^{99}$；

2、计算$x_2=\frac{1}{2}(x_1+x_3)；f_i=f(x_i),i=123$；

3、计算拟合二次曲线的极小值点$\bar{x}$及相应的函数值$f(\bar{x})$；

​		若$\vert\bar{x}-\bar{x_0}\vert<\varepsilon$，则$x^*=\bar{x}，f(x^*)=f(\bar{x})$，停止迭代；

4、根据函数值来缩小区间

5、设$\bar{x_0}=\bar{x}$，转第三步。

注意：一般都是直接给出$x_1，x_2，x_3$，而不需要去中点算$x_2$。



### 混合方法



#### 混合方法基本思想



确定搜索方向：冲初始点出发，大踏步寻找极小值所在的区间

在区间内用插值法近似求解极小值点，以该店为初始点，放小步伐，在此寻找极小值点所在的区间，知道满足停止条件。



#### 混合方法算法描述



1、在第k步，初始点为$x_{0,k}$，步长$\delta_k$

2、计算函数$f(x)$在点$x_{0,k}-\delta_k，x_{0,k}，x_{0,k}+\delta_k$出的函数值

3、根据$f(x_{0,k}-\delta_k)、f(x_{0,k})、f(x_{0,k}+\delta_k)$判断搜索方向

4、沿搜索方向计算一系列点的函数值，直到可以确定极小值出现的区间；

5、在极小值出现的区间内采用插值的方法得到极小值的估计值；

6、以该估计值为$k+1$步的初始点$x_{0,k+1}$，步长为$\delta_{k+1}=K\delta_k$，其中K为步长缩放因子



## 第五章 多为函数无约束梯度方法



### 最速下降法



最速下降法，给定初始值$x_0$，沿下降方向定义为：$d_0=-g_0=-g(x_0)=-\nabla f(x_0)$，沿下降方向下降$\delta_0=\alpha_0\cdot d_0$，其中$\alpha_0$的求解为：$\underset{\alpha}{min}F=f(x_0+\alpha\cdot d)$

不过一般我使用如下公式进行求解
$$
\alpha=-\frac{-g_0^Td_0}{d_0^TH_0d_0}
$$
其中$H_0$是f在$x_0$处的Hessian矩阵，也就是二阶导。

按照如上思想进行更新，$x_{k+1}=x_k+\delta_k=x_k+\alpha_k\cdot d_k=x_k-\alpha_k\cdot g_k$



#### 最速下降法的算法描述



1、输入$x_0$，初始化误差容限$\varepsilon$，设置迭代次数$k=0$;

2、计算函数在$x_k$处的梯度$g_k$，设置下降方向为$d_k=-g_k$；

3、使用线搜索方法找到$\alpha_k$，使得$f(x_k+\alpha\cdot d_k)$最小；

4、设置$x_k+1=x_k+\alpha_k\cdot d_k$，计算$f(x_{k+1})$;

5、若$\vert\vert\alpha_k d_k\vert\vert<\varepsilon$，则：$x^*=x_{k+1}，f(x^*)=f(x_{k+1})$

​      若$\vert\vert\alpha_k d_k\vert\vert\ge\varepsilon$，则设置$k=k+1$并转2.

注意：最速下降法连续的下降方向是正交的



### 牛顿法



使用多维函数的Hessian矩阵的逆来进行求解，即$d=-H^{-1}\cdot g$来表示函数的下降方向。其中，当目标函数为二次函数时，通过一次迭代即可达到最小值点，而若目标函数为非二次函数时，可以通过多次迭代来达到最小值点。

注意：普通牛顿法的步长为1，是固定的；而阻尼牛顿法的步长不定，与其他优化算法相同需要求解。



#### 牛顿法基本步骤



1、初始化，输入$x_0$及误差容限$\varepsilon$，迭代次数$k=0$

2、计算$g_k,H_k$，若$H_k$非正定，通过变换得到$\hat{H_k}>0$

3、计算$H_k^{-1}$，并计算$d_k=-H_k^{-1}g_k$

4、使用线搜索的方法找到$\alpha_k$，使得$f(x_k+\alpha_k d_k)$最小化

5、计算$x_{k+1}=x_k+\alpha_k d_k$，$f_{k+1}=f(x_{k+1})$

6、若$\vert\vert\alpha_k d_k\vert\vert<\varepsilon$，则终止迭代，输出：$x^*=x_{k+1},f(x^*)=f(x_{k+1})$

​     若$\vert\vert\alpha_k d_k\vert\vert>\varepsilon$，则$k=k+1$，返回步骤2

注意：Hessian矩阵为非正定的时候的处理方式如下
$$
\hat{H_k}=\frac{H_k+\beta I_n}{1+\beta}
$$
当$H_k$非正定时，选取$\beta$为一个较大的值，此时有：
$$
\hat{H_k}\approx I_n \qquad d_k\approx -g_k
$$
当$H_k$正定时，选取$\beta$为一个较小的值，此时有：
$$
\hat{H_k}\approx H_k
$$


### 高斯牛顿法



高斯牛顿法时牛顿法的特例，其只能优化形如如下形式的目标函数
$$
\varepsilon(a)=\sum_{i=1}^N \vert\vert f_i(a)-y_i\vert\vert^2
$$
找到一组解$a=[a_1,a_2,...,a_M]^T$，使得上式最小。

对$a_j$求导：
$$
\frac{\partial\varepsilon(a)}{\partial a_j}=
\sum_{i=1}^N2(f_i(a)-y_i)\cdot \frac{\partial f_i(a)}{\partial a_j}
$$
我们记$\varepsilon(a)$的各项简记如下
$$
J=\left[
\begin{matrix}
\frac{\partial f_1(a)}{\partial a_1} & \frac{\partial f_1(a)}{\partial a_2} & ...&\frac{\partial f_1(a)}{\partial a_M} \\
\frac{\partial f_2(a)}{\partial a_1} & \frac{\partial f_2(a)}{\partial a_2} & ...&\frac{\partial f_2(a)}{\partial a_M} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial f_N(a)}{\partial a_1} & \frac{\partial f_N(a)}{\partial a_2} & ...&\frac{\partial f_N(a)}{\partial a_M} \\
\end{matrix}
\right]
&&,
r=\left[
\begin{matrix}
f_1(a)-y_1\\
f_2(a)-y_2\\
\vdots\\
f_N(a)-y_N\\
\end{matrix}
\right]
$$
可以看到，$J$就是$\varepsilon(a)$的雅可比矩阵。
$$
g_F=2J^Tr\quad H_F\approx2J^TJ
$$
使用牛顿法，得到求解极小值点的迭代公式为：
$$
x_{k+1}=x_k-\alpha_k(2J^TJ)^{-1}(2J^Tr)=x_k-\alpha_k(J^TJ)(J^Tr)
$$
注意：高斯牛顿法只能解形如二次的最小二乘法类型的优化问题



## 第六章 共轭方向方法



### 共轭方向法



#### 共轭方向的定义



我们称两个非零向量$d_1,d_2$关于实对成矩阵$H$共轭，若如下式成立：
$$
d_1^THd_2=0
$$
我们称一个向量集合$\{d_0,d_1,...,d_k\}$关于实对称矩阵$H$共轭，若：
$$
d_i^THd_j=0
$$
注意：当H为单位阵时，共轭关系就变成了垂直（正交）关系，所以垂直是一种特殊的共轭。

对称矩阵H的一组特征向量为共轭方向



#### 共轭方向的线性不相关性

若非零向量$d_0,d_1,...,d_k$组成关于正定矩阵H的共轭向量集合，则它们线性无关。



#### 共轭方向法



若${d_0,d_1,...,d_k}$是关于正定矩阵$H$的一组共轭向量，且目标函数为二次函数，即：
$$
\min_xf(x)=a+x^Tb+\frac{1}{2}x^THx
$$
那么从任一初始点$x_0$出发，序列
$$
x_{k+1}=x_k+\alpha_kd_k\quad\alpha_k=-\frac{g_k^Td_k}{d_k^THd_k}\quad g_k=b+Hx_k
$$
收敛到二次函数唯一的极小值点$x^*$。

注意：一般不使用最基本的共轭方向法，因为共轭方向集的选取不唯一，并且一般的共轭方向法只适用于二次函数，通常使用改良方法FR共轭方向法。



### 非二次函数极小化(FR共轭方向法)



1、输入$x_0$和误差容限$\varepsilon$；

2、设置$k=0$，计算梯度$g_0$，设置$d_0=-g_0$；

3、通过线搜索方法找到$\alpha_k$，使得$f(x_k+\alpha d_k)$极小化，同时使$x_{k+1}=x_k+\alpha_k d_k$

4、若$\vert\vert\alpha_kd_k\vert\vert<\varepsilon$，输出$x^*=x_{k+1}$且$f(x^*)=f_{k+1}$，停止计算

5、若$k=n-1$，设置$x_0=x_{k+1}$转到第二步；

6、计算$g_k+1,\beta_k=\frac{\vert\vert g_{k+1}\vert\vert^2}{\vert\vert g_{k}\vert\vert^2}$，计算$d_{k+1}=-g_{k+1}+\beta_kd_k$，置$k=k+1$，转第三步

注意：算法的流程通常为，首先使用最速下降法来完成第一次迭代，然后计算新点的梯度值，使用公式$\beta_k=\frac{\vert\vert g_{k+1}\vert\vert^2}{\vert\vert g_{k}\vert\vert^2},d_{k+1}=-g_{k+1}+\beta_kd_k$，找到第二次迭代的下降方向，然后通过线搜索完成第二次迭代。算法通常在两次迭代内完成。



## 第七章 拟牛顿方法



拟牛顿法的计算流程一般如下

首先通过最速下降法来完成第一次迭代，然后通过更新公式来找到第二次迭代的下降方向，通过线搜索来完成第二次迭代，算法通常在两次迭代后达到最小值点。



### 一阶拟牛顿法



一阶拟牛顿法的更新公式为
$$
S_{k+1}=S_{k}+\frac{(\Delta x_k-S_k\Delta g_k)(\Delta x_k-S_k\Delta g_k)^T}{\Delta g_k^T(\Delta x_k-S_k\Delta g_k)}
$$
其中
$$
\Delta x_k=x_{k+1}-x_k\quad\Delta g_k=g_{k+1}-g_k\quad S_1=I
$$
方向更新公式如下：
$$
d_k=-S_kg_k
$$
之所以第一次使用最速下降法，是因为第一次$S_1=I$，这样得到的$d_1=-g_1$，与最速下降法的下降方向相同。

注意：使用一阶牛顿法得到的$S_k$并不能保证正定，所以我们需要判断$\beta_k=\Delta g_k^T(\Delta x_k-S_k\Delta g_k)$的正负，若为零或者负值，则更新公式变为：$S_{k+1}=S_k$



### DFP法



相较于一阶拟牛顿法，DFP法一定能保证$S_1,S_2,...,S_n$正定

其更新公式为：
$$
S_{k+1}=S_k+\frac{\Delta x_k \Delta x_k^T}{\Delta x_k^T\Delta g_k}-\frac{S_k\Delta g_k\Delta g_k^TS_k}{\Delta g_k^TS_k\Delta g_k}
$$
注意：由于要保证相乘结果为矩阵，分子上面转置在后面，而分母的转置在前面，这样能保证三项的维度相同，加减号为先加后减。

一般使用DFP法进行优化，而不适用一阶拟牛顿法。



## 第九章 无约束优化问题的应用



### 模式相似的定义



两个模式相似，其中一个模式可以由另外一个模式经过旋转、缩放和平移得到。

其中记模式一：
$$
\rho=\{p_1,p_2,...,p_n\},p_i=[p_{i1},p_{i2}]^T
$$
模式二：
$$
\tilde{\rho}=\{\tilde{p_1},\tilde{p_2},...,\tilde{p_n}\},\tilde{p_i}=[\tilde{p_{i1}}，\tilde{p_{i2}}]^T
$$
模式相似：
$$
\tilde{p_i}=\eta
\left[
\begin{matrix}
cos\theta & -sin\theta \\
sin\theta & cos\theta
\end{matrix}
\right]p_i+
\left[
\begin{matrix}
r_1 \\
r_2
\end{matrix}
\right]
$$
使用变量替换：
$$
a=\eta cos\theta,\quad b=\eta sin\theta
$$
旋转、缩放及平移转换为：
$$
\tilde{p_i}=
\left[
\begin{matrix}
a & -b \\
b&a
\end{matrix}
\right]p_i
+
\left[
\begin{matrix}
r_1\\
r_2
\end{matrix}
\right]
$$
相似度量
$$
\min_x\vert\vert\rho-\tilde{\rho}\vert\vert_F^2=\sum_{i=1}^n\vert\vert p_i-\tilde{p_i}\vert\vert^2
$$


## 第十章 约束优化基本理论



约束优化问题的一般形式：
$$
min f(x)\\
s.t.\quad a_i(x)=0\quad for\quad i=1,2,...,p\\
\quad\quad\quad c_i(x)\ge0\quad for\quad j=1,2,...,q
$$


### 正则点



正则点$x^*$应满足，该点满足等式约束$a(x^*)=0$，且下列向量$\nabla a_1(x),\nabla a_2(x),...,\nabla a_p(x)$线性无关。

即雅可比矩阵$[\nabla a_1(x),\nabla a_2(x),...,\nabla a_p(x)]^T$行满秩。



### 活跃约束与非活跃约束



活跃约束(active constraints)与非活跃约束(inactive constraints)是针对不等约束来说的，其中活跃约束是不等约束中起作用的约束，即点位于约束边界的约束。而非活跃约束是不等约束中不起作用的约束，即点位于约束内部的约束。

即对于活跃约束来说：
$$
c(x)=0
$$
而对于非活跃约束来说:
$$
c(x)>0
$$


### 线性规划问题的标准形式


$$
\min f(x)=c^Tx\\
s.t.\quad Ax=b\\
\quad\quad\quad x\ge0
$$
非标准形式可以化为标准形式。

转化方式如下：

对于无约束变量，即若$x_1$为无约束变量，我们可以令$x_1=x_1^+-x_1^-$，其中$x_1^+,x_1^-\ge0$。

对于不等约束，例如不等约束$g(x)\ge0$，可以添加松弛变量$x_{n+1}$，变成$g(x)-x_{n+1}=0,x_{n+1}\ge0$，转化为等式约束。



### 拉格朗日乘子



拉格朗日函数如下
$$
L(x,\lambda)=f(x)-\sum_{i=1}^p\lambda_ia_i(x)
$$
其中$a_i(x）$为等式约束，这样就将约束优化问题转化为无约束优化问题。

在极小值点处的梯度应该满足：
$$
\nabla f(x^*)=\sum_{i=1}^p\lambda_1^*\nabla a_i(x^*)
$$



### 切平面与法平面



切平面与若干等式约束平面$a(x)$相切，其方程为：
$$
T_{x^*}:J_a(x^*)(x-x^*)=0
$$
法平面与切平面正交，其方程为：
$$
N_{x^*}:x-x^*=\sum_{i=1}^p\alpha_i\nabla a_i(x^*)\quad for\quad \alpha_i\in R
$$


### 一阶必要条件KKT条件



含有不等约束的优化问题极小值点的一阶必要条件：

若有k个不等式约束变为active，则梯度应该满足的条件为:
$$
\nabla f(x^*)=\sum_{i=1}^p\lambda_i^*\nabla a_i(x^*)+\sum_{k=1}^K\mu_{jk}^*\nabla c_{jk}(x^*)
$$
$\lambda_i^*,\mu_{jk}^*$为相应的拉格朗日乘子。

在等式约束中，拉格朗日乘子可以为正值，负值。而在active的不等约束中，拉格朗日乘子必须为非负。



#### KKT条件



若$x^*$是优化问题的极小值点，且该点是active约束的一个正则点，则有：

1、$a_i(x^*)=0\quad for\quad 1\le i\le p,c_j(x^*)\ge0\quad for\quad 1\le j\le q$

2、存在拉格朗日乘子$\lambda_i^*,1\le i\le p$及$\mu_j^*,1\le j\le q$使得
$$
\nabla f(x^*)=\sum_{i=1}^p\lambda_i^*\nabla a_i(x^*)+\sum_{k=1}^K\mu_{jk}^*\nabla c_{jk}(x^*)
$$
3、$\lambda_i^*a_i(x^*)=0\quad for\quad 1\le i\le p,\mu_j^*c_j(x^*)=0\quad for\quad 1\le j\le q$

4、$\mu_j^*\ge0\quad for\quad1\le j\le q$

注意：在解KKT条件（卷子上写的是K-T条件）的题目时，通常要对不等约束进行分情况讨论，按照不同的积极约束进行讨论，最后得到满足条件的K-T点。



对于凸优化问题，局部极小值点也是全局极小值点。

若$x^*$是一个正则点，前满足KKT条件，那么$x^*$是一个全局极小值点。



### 对偶特性



线性规划的对偶问题可以通过查表的方式直接进行转换

这里就不插入表格了，主要就是将原问题中目标函数的系数变为对偶问题中约束的常数，将原问题中约束条件系数的转置变成对偶问题中约束条件的系数，将原问题中约束条件的常数变成对偶问题中目标函数的系数。不等号的方向变反，若无约束变成等号，等号变成无约束。

对于非线性规划问题，对偶问题过于复杂，这里不讨论。



## 线性规划-单纯性算法

