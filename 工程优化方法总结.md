# 工程优化方法总结



## 第二章 基础知识



### 多元函数的梯度



以f的n个偏导数为分量的向量称为f在x处的梯度，记为

$$
\nabla f(x)=(\frac{\partial f(x)}{\partial x_1},\frac{\partial f(x)}{\partial x_1},...\frac{\partial f(x)}{\partial x_1})^T
$$
也称作f关于x的一阶导数。



梯度方向是函数具有最大变化率的方向，**即：沿地图方向的方向导数最大。**



### 方向导数


$$
\frac{\partial f(x_0)}{\partial p}=\lim_{t \to 0^+}\frac{f(x_0+p)-f(x_0)}{t}=\lim_{t \to 0^+}\frac{f(x_0+te)-f(x_0)}{t}
$$



当方向导数大于0时，表示$f(x)$在$x_0$处沿$p$方向上升；反之，当方向导数小于0时，表示$f(x)$在$x_0$处沿$p$方向下降。



函数升降速度的快慢有方向导数的绝对值大小来决定，绝对值越大生姜速度越大，也称为$f(x)$在$x_0$处沿$p$方向的变化率。



**推论：**若$\nabla f(x_0)^T p<0$，则$p$是函数$f(x)$在$x_0$处的下降方向。

​			若$\nabla f(x_0)^T p>0$，则$p$是函数$f(x)$在$x_0$处的上升方向。

通过这个推论，我们可以判断在某一点处沿某一方向是上升方向还是下降方向。



### 多元函数的Hessian矩阵



多元函数的二阶导数即为Hessian矩阵$\nabla^2f(x)$
$$
\nabla^2f(x)=\nabla(\nabla f(x))= \left[
\begin{matrix}
\frac{\partial^2f(x)}{\partial x_1^2} & \frac{\partial^2f(x)}{\partial x_1 \partial x_2} & ...
\frac{\partial^2f(x)}{\partial x_1 \partial x_n} \\
\frac{\partial^2f(x)}{\partial x_2 \partial x_1} & \frac{\partial^2f(x)}{\partial x_2^2} & ...
\frac{\partial^2f(x)}{\partial x_2 \partial x_n} \\
\frac{\partial^2f(x)}{\partial x_n \partial x_1} &
\frac{\partial^2f(x)}{\partial x_n \partial x_2} & ...
\frac{\partial^2f(x)}{\partial x_n^2}

\end{matrix}
\right]
$$


使用顺序主子式来判断Hessian矩阵是否正定。



### 泰勒展开



以二元函数在某一点处的二阶泰勒展开为例
$$
f(x_1+\delta_1,x_2+\delta_2)=f(x_1,x_2)+\frac{\partial f}{\partial x_1}\delta_1+\frac{\partial f}{\partial x_2}\delta_2 + \frac{1}{2}(\frac{\partial^2 f}{\partial x_1^2}\delta_1^2 + \frac{2\partial^2 f}{\partial x_1\partial x_2}\delta_1 \delta_2+\frac{\partial^2 f}{\partial x_2^2}\delta_2^2) + o(\Vert \delta \Vert^2)
$$



### 可行方向



给定可行域$\Omega$内的一点$x$，若存在一个常量$\alpha>0$，对于任意一个方向向量，有$x+\alpha d\in\Omega$，对所有的$0\leq \alpha\leq\hat{\alpha}$都成立，那么我们成$d$为点$x$处的一个可行方向。



### 二元函数的极值判别条件



#### 一阶必要条件



1、若目标函数连续可导，点$x^*$式一个局部极小值点，那么有：
$$
g(x^*)\cdot d \geq0
$$
其中，d是点$x^*$处的任意一个可行方向。

2、若$x^*$位于可行域$\Omega$内部（意味着任意方向均为可行方向）,那么有
$$
g(x^*)=0
$$
即，若极小值点位于边界，那么其任意方向均为上升方向。而若极小值点位于可行域内部，则其一阶导数为0。



#### 二阶必要条件



1、若$f(x)$二阶连续可到，那么当$x^*$是一个局部极小值点时，对于$x^*$处的任一可行方向：

1）$g(x^*)^T\cdot d\ge0$

2)若$g(x^*)^T\cdot d=0$，那么有$d^TH(x^*)d\ge0$。

2、若$x^*$是可行域上的内点，那么，$x^*$是一个极小值点的必要条件是：

1)$g(x^*)=0$

2)$d^TH(x^*)d\ge 0$对于所有的向量$d$。

即所有的二阶导与方向的乘积均要大于等于0。



#### 二阶充分条件



若$f(x)$二阶连续可导，点$x^*$位于优化问题可行域的内部，则若下列两个条件成立，$x^*$是一个强局部最小点。

1、$g(x^*)=0$

2、$H(x^*)$正定



### 凸集



对于集合$R_c$中的任意两点$x_1,x_2$以及任意$\alpha$满足$0<\alpha<1$，若点$x=\alpha\cdot x_1+(1-\alpha)\cdot x_2$依然在$R_c$中，那么我们称$R_c$为凸集。

设A,B是凸集，则$A\cap B、A+B、A-B$也是凸集，**而$A\cup B$不一定是凸集。**



### 凸函数



设集合D为凸集，函数$f:D\rightarrow R$对于任意两点$x_1,x_2$和$0<\alpha<1$，有
$$
f(\alpha\cdot x_1+(1-\alpha)\cdot x_2)\le\alpha\cdot f(x_1)+(1-\alpha)\cdot f(x_2)
$$
成立，则成$f(x)$为凸集D上的凸函数。



### 凸函数的性质



#### 凸函数与梯度的关系



设$f(x)$在凸集$R_c$上一阶连续可到，那么$f(x)$是定义在凸集$R_C$上的凸函数当且仅当：
$$
f(x_1)\ge f(x) + g(x)^T(x_1-x)
$$
对所有的$x,x_1\in R_c$都成立，这里$g(x)=\nabla f(x)$



#### 凸函数与Hessian矩阵的关系



设$f(x)$在凸集$R_c$上二阶连续可导，$f(x)$是凸函数当且仅当对于的Hessian矩阵式半正定的。



### 凸规划问题的性质



设$f(x)$式定义在凸集$R_c$上的凸函数，那么：

$f(x)$的局部极小值点即为全局极小值点。



## 第三章 优化算法的一般特性



### 下降函数



假设一系列点序列是由某一算法A产生的，我们想找到一个连续的实值函数$D(x)$，使$D(x_{k+1})\le D(x_k)$，称$D(x)$为下降函数。（自己的理解）



定义：

设$S\subset X$包含问题的解，假定算法A是定义在空间X上的算法，即$x_{k+1}=A(x_k)$，我们成一个连续的实值函数$D(x)$为下降函数若该函数满足一下条件

1、若$x_k\notin S$，对于$x_{x+1}\in A(x_k)$，有$D(x_{k+!})<D(x_k)$；

2、若$x_k\in S$，对于$x_{k+1}\in A(x_k)$，有$D(x_{k+1})\le D(x_k)$。



注意：要区分算法A和下降函数D，算法A是产生一系列点序列的函数，而下降函数是在该序列上满足下降条件的函数。



### 算法收敛性

由算法A产生的点序列随着迭代次数的无线增加，点序列趋近于一个点。类似于极限的概念。（自己的理解）



定义：

设A为空间X上的一个算法，那么从初始点$x_0$出发，可由$x_{k+1}\in A(x_k)$得到一个无限长序列$\{x_k\}_{k=0}^\infty$。假如对应的优化问题的解集S和下降函数$D(x)$存在，使得以下条件成立，则$\{x_k\}_{k=0}^\infty$极限收敛到解集中一点。

1、所有$x_k$都包含在X的一个闭的、有界子集内；

2、$D(x_k)$为下降函数；

3、算法A在$x\in X$和$x\notin S$内是封闭的。



注意：验证算法收敛性注意三点，点集封闭，算法封闭，存在下降函数。



### 收敛阶次

收敛阶次定义在优化算法产生的无限序列$\{x_k\}_{k=0}^\infty$上。针对该无限序列，收敛阶次的定义满足
$$
0\le\beta<\infty 其中\beta =\lim_{k\rightarrow\infty}\frac{\vert x_{k+1}-\hat{x}\vert}{\vert x_k-\hat{x}\vert^p}
$$


的最大非负整数p，同时称$\beta$为收敛率。

注意：$\hat{x}$为序列收敛的点，一般都能一眼看出来。



## 第四章 一维函数优化



### 二分法



从搜索区间$[a,b]$的中点$x_c$出发，找$x_c-\varepsilon、x_c+\varepsilon$，其中$\varepsilon>0$，通过比较$f(x_c-\varepsilon)、f(x_c+\varepsilon)$的函数值，来缩小区间，不断重复，直到区间长度满足搜索精度要求。



#### 二分法算法步骤



已知：单峰函数$f(x)$，区间$[x_L,x_U]$包含函数的极值点。

1、计算极值点所在区间中点，记为$x_c$

2、计算函数在$f(x_c-\varepsilon)$和$f(x_c+\varepsilon)$处的函数值，其中$\varepsilon>0$

3、由$f(x_c-\varepsilon)$和$f(x_c+\varepsilon)$的大小关系，缩小极值点所在的区间

4、重复上述步骤，直到区间长度满足精度要求

注意：这里$\varepsilon$很小



每一次迭代都使得区间的长度减半，这样经过k次迭代，区间长度降为：
$$
I_k=(\frac{1}{2})^k\cdot I_0
$$


#### 三种停止条件



1、极值点的精度：$\vert x-x^*\vert\le\zeta$；其中$x^*$为准确的最小值点，$\zeta$为设定的误差容限。（该条件需要预先知道最小值点，一般用$\vert x_{k+1}-x_k\vert\le\zeta$）

2、极值的精度：$\vert f(x_{k+1})-f(x_k)\vert\le\varepsilon$（理想：$\vert f(x_{k+1})-f(x^*)\vert\le\varepsilon$）

3、迭代次数：即当迭代次数满足一定数目时，迭代停止



### 斐波那契法



第一次在区间$[a,b]$内找两个点$x_1、x_2$，使得$x_2-a=b-x_1$。通过比较$f(x_1)、f(x_2)$的函数值来缩小区间。假设我们将$[a,x_1]$部分舍弃，现在有$x_1、x_2、b$三个点，令$a=x_1、x_1=x_2、b=b$，下次迭代只需找一个新点$x_2$，使得新的四个点满足$x_2-a=b-x_1$即可。

二分法每次迭代都需要找区间内两点，计算两点的函数值。斐波那契法只在第一次迭代时找区间内两点，余下迭代时只需新计算一个点的函数值。



#### 斐波那契法算法步骤



假设第k次迭代，区间为$[x_{Lk},x_{Uk}]$，长度为$I_k$。

1、在区间$[x_{Lk},x_{Uk}]$内取两点$x_{ak},x_{bk}$，使得区间$[x_{Lk},x_{bk}]$和$[x_{ak},x_{Uk}]$长度相等，即：$I_{k+1}^L=I_{k+1}^R$

2、$x_{ak}$或$x_{bk}$刚好为下一次迭代区间中的一个点

3、上述迭代过程满足：$I_k=I_{k+1}+I_{k+2}$，即迭代的区间长度应满足斐波那契数列。



#### 斐波那契法的特点



1、迭代开始前，就要根据区间长度和误差容限确定斐波那契数列的长度，确定斐波那契数列长度的依据：
$$
I_n=\frac{I_1}{F_n}
$$
2、如果优化问题的停止条件是类似$\vert f(x_{k+1})-f(x_k)\vert\le\varepsilon$，那么斐波那契法不太好处理。

3、斐波那契法收敛到相同精度的极值点，需要计算函数值的点数比二分法少，但是迭代的次数多。



### 黄金分割法



第一次在搜索区间$[a,b]$内找两个点$x_1=b-0.618(a-b)、x_2=a+0.618(a-b)$，分别计算$f(x_1)、f(x_2)$的函数值，通过比较函数值来缩小区间，并使上次迭代时的两个点中的一点成为下次迭代时的一个点。（思想跟斐波那契法差不多，区别在于找点的方法）



#### 黄金分隔法算法步骤



1、初始化：函数$f(x)$，包含极值点的区间$[x_{L，1}，x_{U,1}]$，误差限$\varepsilon$。

2、计算：$I_1=x_{U,1}-x_{L,1}，I_2=I_1/K，x_{a,1}=x_{U,1}-I_2，x_{b,1}=x_{L,1}-I_2$

3、计算：$I_{k+2}=I_{k+1}/K$，根据$f(x_{L,k})，f(x_{a,k})，f(x_b,k)，f(x_{U,k})$的值更新$x_{L,k+1}，x_{a,k+1}，x_{b,k+1}，x_{U,k+1}$。

4、若$I_k<\varepsilon$，结束迭代过程，计算：
$$
x^*=
\begin{cases}
\frac{1}{2}(x_{a,k+1}+x_{U,k+1})(f_{a,k+1>f_{b,k+1}})\\
\frac{1}{2}(x_{a,k+1}+x_{b,k+1})(f_{a,k+1=f_{b,k+1}})\\
\frac{1}{2}(x_{L,k+1}+x_{b,k+1})(f_{a,k+1<f_{b,k+1}})\\
\end{cases}
$$
(其实只要满足了误差容限，区间内任意一点都能够代表极小值点)

5、设置$k=k+1$，转至第三步。



### 二次内插方法



通过搜索区间内选三个点，来拟合二次曲线，通过二次曲线的极值来进行迭代。



#### 二次内插法基本思想



已知$x_1，x_2，x_3$以及相应的函数值，可以用二次函数近似目标曲线，通过解二次函数极小值近似原函数极小值。



#### 二次内插法步骤

1、输入$x_1，x_3$以及误差容限$\varepsilon$；设$\bar{x_0}=10^{99}$；

2、计算$x_2=\frac{1}{2}(x_1+x_3)；f_i=f(x_i),i=123$；

3、计算拟合二次曲线的极小值点$\bar{x}$及相应的函数值$f(\bar{x})$；

​		若$\vert\bar{x}-\bar{x_0}\vert<\varepsilon$，则$x^*=\bar{x}，f(x^*)=f(\bar{x})$，停止迭代；

4、根据函数值来缩小区间

5、设$\bar{x_0}=\bar{x}$，转第三步。

注意：一般都是直接给出$x_1，x_2，x_3$，而不需要去中点算$x_2$。



### 混合方法



#### 混合方法基本思想



确定搜索方向：冲初始点出发，大踏步寻找极小值所在的区间

在区间内用插值法近似求解极小值点，以该店为初始点，放小步伐，在此寻找极小值点所在的区间，知道满足停止条件。



#### 混合方法算法描述



1、在第k步，初始点为$x_{0,k}$，步长$\delta_k$

2、计算函数$f(x)$在点$x_{0,k}-\delta_k，x_{0,k}，x_{0,k}+\delta_k$出的函数值

3、根据$f(x_{0,k}-\delta_k)、f(x_{0,k})、f(x_{0,k}+\delta_k)$判断搜索方向

4、沿搜索方向计算一系列点的函数值，直到可以确定极小值出现的区间；

5、在极小值出现的区间内采用插值的方法得到极小值的估计值；

6、以该估计值为$k+1$步的初始点$x_{0,k+1}$，步长为$\delta_{k+1}=K\delta_k$，其中K为步长缩放因子



## 第五章 多为函数无约束梯度方法



### 最速下降法



最速下降法，给定初始值$x_0$，沿下降方向定义为：$d_0=-g_0=-g(x_0)=-\nabla f(x_0)$，沿下降方向下降$\delta_0=\alpha_0\cdot d_0$，其中$\alpha_0$的求解为：$\underset{\alpha}{min}F=f(x_0+\alpha\cdot d)$

不过一般我使用如下公式进行求解
$$
\alpha=-\frac{-g_0^Td_0}{d_0^TH_0d_0}
$$
其中$H_0$是f在$x_0$处的Hessian矩阵，也就是二阶导。

按照如上思想进行更新，$x_{k+1}=x_k+\delta_k=x_k+\alpha_k\cdot d_k=x_k-\alpha_k\cdot g_k$



#### 最速下降法的算法描述



1、输入$x_0$，初始化误差容限$\varepsilon$，设置迭代次数$k=0$;

2、计算函数在$x_k$处的梯度$g_k$，设置下降方向为$d_k=-g_k$；

3、使用线搜索方法找到$\alpha_k$，使得$f(x_k+\alpha\cdot d_k)$最小；

4、设置$x_k+1=x_k+\alpha_k\cdot d_k$，计算$f(x_{k+1})$;

5、若$\vert\vert\alpha_k d_k\vert\vert<\varepsilon$，则：$x^*=x_{k+1}，f(x^*)=f(x_{k+1})$

​      若$\vert\vert\alpha_k d_k\vert\vert\ge\varepsilon$，则设置$k=k+1$并转2.

注意：最速下降法连续的下降方向是正交的

